<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Irwan Bello</title>
  
  <meta name="author" content="Irwan Bello">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Irwan Bello</name>
              </p>
              <p>
                <i>Hi! I'm an AI researcher living in SF and part of the founding team at <a href="character.ai">Character</a></i>.
              </p>
              <p>
                <b><a href="https://www.character.ai">[Character]</a></b>
                Building the next-generation of open-ended dialog agents. Stay tuned for upcoming news.
              </p>
              <p>
                <b><a href="https://research.google/teams/brain/">[Google Brain]</a></b>
                I spent 5.5 years as a Research Scientist at Google Brain working on Artificial Intelligence, Large Language Models (LLMs) and Computer Vision.
              </p>
              <p style="margin-left: 10px">
                <b>- LLMs -</b> 
                My research focused on making large language models cheaper to work with - via <a href="https://arxiv.org/abs/2202.08906">sparsity</a>, adaptive computation and better distributed computing infrastructure.
                <br>
                <br>
                <b>- Computer Vision -</b> 
                I authored some of the pioneering work on <a href="https://arxiv.org/abs/1904.09925">Attention for Vision</a>, notably training the first ever fully-attentional vision model. 
                I proposed <a href="https://arxiv.org/abs/2102.08602">LambdaNetworks</a> as a faster alternative which resulted in 4-10x speed-ups over the prior state-of-the-art.
                I also worked on simple baselines for <a href="https://arxiv.org/abs/2103.07579">image classification</a> and <a href="https://arxiv.org/abs/2109.01696">video classification</a>.
                <br>
                <br>
                In the past, I proposed 
                <a href="https://arxiv.org/abs/1611.09940">Neural Combinatorial Optimization</a> with applications to
                <a href="https://arxiv.org/abs/1709.07417">AutoML</a> and
                <a href="https://arxiv.org/abs/1810.02019">Ranking</a>.
                I also contributed to Google products such as <a href="https://cloud.google.com/automl">AutoML</a>, <a href="https://blog.google/products/search/introducing-mum/">MuM</a> and Youtube.
              </p>
              <b>[Stanford]</b> Before Google, I spent wonderful years at Stanford as a grad student between the stats and the CS departments, after obtaining my M.S in Applied Math at Ecole Centrale Paris.
              <br>
              <br>
              <b>[AI & Startups]</b> I advise a few select AI startups with a focus on LLMs and their applications (<i>reach out if you're interested!</i>).
              <!-- <br> -->
              <!-- <br> -->
              <!-- When I'm not thinking quantitavely you can probably find me doing something music related. -->
              <br>
              <p>
                <b>[Talks & Press]</b>
                <p style="margin-left: 20px">
                <ul>
                <li><a href="https://twimlai.com/podcast/twimlai/mixture-of-experts-and-trends-in-large-scale-language-modeling/">TwiML: Mixture-of-Experts and Trends in LLMs</a></li>
                <br>
                <li><a href="https://www.youtube.com/watch?v=U8J32Z3qV8s&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=5"> Stanford CS25: Mixture-of-Experts Sparse Transformers</a></li>
                <br>
                <li><a href="https://www.youtube.com/watch?v=3qxJ2WD8p4w">Yannic Kilcher's review of LambdaNetworks</a></li>
                <br>
                <li><a href="https://www.youtube.com/watch?v=l5ab290no8c&t=8s">London ML Meetup talk</a></li>
                <br>
                <li><a href="https://www.tf1info.fr/high-tech/irwan-bello-de-centrale-paris-stanford-a-l-intelligence-artificielle-de-google-brain-2086812.html">An old profile (in French)</a></li>
                <ul>
                </p>
              </p>
              <br>
              (Last update: September 2022)
              <br>
              <br>
              </p>
              <p style="text-align:center">
                <a href="mailto:firstname.lastname@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=mY6p8gcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/IrwanBello">Twitter</a> &nbsp/&nbsp
              <a href="https://soundcloud.com/irwan-bello">Soundcloud</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%;padding-bottom:40%">
            <!-- <td style="padding-left:2.5% 2.5% 10% 0%;width:40%;max-width:40%;padding-top:10%"> -->
            <!-- <td style="width:40%;max-width:40%;padding-top:10%"> -->
              <a href="images/me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr bgcolor="#ffffd0">
          <td>
            <p>
              <a href="https://arxiv.org/abs/2202.08906">
              <b>ST-MoE: Designing Stable and Transferable Sparse Expert Models</b>
              </a>
              <br> 
              <i>Barret Zoph*, <strong>Irwan Bello*</strong>, Sameer Kumar, Nan Du, Yanping Huang,  Jeff Dean, Noam Shazeer, William Fedus*.</i>
              <p></p>
              <p>
              Sparse Mixture of Experts (MoE) suffer from training instabilities and finetuning issues at scale.
              We design improved methods for modeling, pretraining and finetuning sparse models.
              We introduce the Stable and Transferable Mixture-of-Experts (ST-MoE) and scale it to 269B sparse parameters - the largest sparse encoder-decoder model ever trained.
              Our largest model, ST-MoE-32B sets a new state-of-the-art on many NLP benchmarks including SuperGLUE and ARC Easy / ARC Challenge.
              </p>
            </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/2109.01696">
                <b>Revisiting 3D ResNets for Video Recognition</b>
              </a>
              <br>
              <i>Xianzhi Du, Yeqing Li, Yin Cui, Rui Qian, Jing Li, <strong>Irwan Bello</strong>.</i>
              <p></p>
              <p>
              3D ResNet-RS, obtained through improved training and scaling strategies, achieves competitive performance on Kinetics and a large Web Video Text dataset.
              </p>
            </p>
          </td>
        </tr>

        <tr bgcolor="#ffffd0">
          <td>
            <p>
              <a href="https://arxiv.org/abs/2103.07579">
                <b>Revisiting ResNets: Improved Training and Scaling Strategies <font color="dark green">[Neurips 2021 Spotlight]</font></b> 
              </a>
              <br>
              <i><strong>Irwan Bello</strong>, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, Barret Zoph.</i>
              <br>
              <a href="https://github.com/tensorflow/tpu/tree/master/models/official/resnet">[Github]</a>
              <a href="https://cloud.google.com/tpu/docs/tutorials/resnet-rs-2.x">[Google Cloud]</a>
              <a href="https://wandb.ai/wandb_fc/pytorch-image-models/reports/Revisiting-ResNets-Improved-Training-and-Scaling-Strategies--Vmlldzo2NDE3NTM">[Blog posts 1</a>,
              <a href="https://gdude.de/blog/2021-03-15/Revisiting-Resnets">2</a>,
              <a href="https://andlukyane.com/blog/paper-review-resnetsr">3]</a>
              <p></p>
              <p>
                This paper disentangles the impact of architectures vs training and scaling - revealing that improvements in image classification have been primarily driven by improved training and scaling.
                Identifies general scaling strategies that improve vision models across training setups and introduces SOTA competitive ResNet-RS.
                <br> The training and scaling strategies have been used in multiple recent architectures
                [<a href="https://arxiv.org/abs/2102.08602">1</a>,
                 <a href="https://arxiv.org/abs/2102.06171">2</a>,
                 <a href="https://arxiv.org/abs/2101.11605">3</a>]
                 and the work has inspired follow-up research on scaling and regularizing architectures, e.g.
                <a href="https://arxiv.org/abs/2107.00057">RetinaNet-RS</a>,
                <a href="https://arxiv.org/abs/2109.01696">3D-ResNet-RS</a>.
              </p>
            </p>
          </td>
        </tr>

        <tr bgcolor="#ffffd0">
          <td>
            <p>
              <a href="https://arxiv.org/abs/2102.08602">
                <b>LambdaNetworks: Modeling Long-Range Interactions without Attention <font color="dark green">[ICLR 2021 Spotlight]</font></b>
              </a> 
              <br>
              <i><strong>Irwan Bello.</strong></i>
              <br>
              <a href="https://github.com/lucidrains/lambda-networks">
                [Github]
              </a>
              <a href="https://www.youtube.com/watch?v=3qxJ2WD8p4w">
                [Yannic Kilcher's review]
              </a>
              <a href="https://www.youtube.com/watch?v=l5ab290no8c&t=8s">
                [London ML Meetup talk]
              </a>
              <a href="https://medium.com/analytics-vidhya/lambdanetworks-modeling-long-range-interactions-without-attention-337771f42b6f">[Blog posts 1,</a>
              <a href="https://medium.com/syncedreview/iclr-2021-submission-lambda-networks-achieve-sota-accuracy-save-massive-memory-c9eba4be057d"> 2,</a>
              <a href="https://vaclavkosar.com/ml/Lamda-Networks-Transform-Self-Attention"> 3]
              </a>
              <br> <i>This paper sits at the intersection of linear attention and fully-attentional vision models (concurrent to Vision Transformers).</i>
              <p></p>
              <p>
                Introduces <i>lambda layers: a scalable alternative to self-attention</i>.
                 Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs. 
                LambdaResNets are <b>3.2 - 4.4x</b> faster than EfficientNets in supervised learning, and <b>~9x</b> than EfficientNet and ViT in large-scale semi-supervised learning.
              </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/2010.03019">              
                <b>Global Self-Attention Networks for Image Recognition</b> 
              </a> 
              <br>
              <i><strong>Irwan Bello*</strong>, Zhuoran Shen*, Raviteja Vemulapalli, Xuhui Jia, Ching-Hui Chen.</i>
              <p></p>
              <p>
                Combining linear attention and axial attention yields an attention mechanism that can efficiently attend to higher resolution images.
              </p>
            </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/1906.05909">              
                <b>Stand-alone Self-Attention in Vision Models</b> 
              </a>
              <br>
              <i> Prajit Ramachandran*, Niki Parmar*, Ashish Vaswani*, <strong>Irwan Bello</strong>, Anselm Levskaya, Jonathon Shlens.</i>
              <br>
              [NeurIPS 2019]
              <p></p>
              <p>
                Study of fully atttentional networks on image classification and object detection.
                A simple procedure of replacing all spatial convolutions with self-attention in ResNets produces a fully self-attentional model that outperforms its convolutional counterpart on image classification and object detection, while being more computationally efficient.
                These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.
              </p>
            </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/1904.09925">
                <b>Attention Augmented Convolutional Networks</b> 
              </a>
              <br>
              <i> <strong>Irwan Bello</strong>, Barret Zoph, Ashish Vaswani, Jonathon Shlens, Quoc Le.</i>
              <br>
              [ICCV 2019]
              <p></p>
              <p>
                Trained <i>the first fully attentional image classifier</i> and showed that self-attention is a competitive replacement to convolutions for image classification.
                Hybrid architectures which combine self-attention and convolution yields sizable improvements on image classification and object detection.
              </p>
            </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/1810.02019">
                <b>Seq2slate: Re-ranking and Slate Optimization with RNNs</b>                 
              </a>
              <br>
              <i> <strong>Irwan Bello</strong>, Sayali Kulkarni, Sagar Jain, Craig Boutilier, Ed Chi, Elad Eban, Xiyang Luo, Alan Mackey, Ofer Meshi.</i>
              <p></p>
              <p>
                Learning to rank with Pointer Networks outperforms pointwise, pairwise and listwise ranking baselines on academic datasets and in offline experiments.
              </p>
            </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/1808.02822">
                <b>Backprop Evolution</b>
              </a>
              <br>
              <i> Maximilian Alber*, <strong>Irwan Bello*</strong>, Barret Zoph, Pieter-Jan Kindermans, Prajit Ramachandran, Quoc Le.</i>
              <p></p>
              <p>
                Starting from random or known propagation rules, evolution searches for backpropagation variants that maximize generalization performance.
              </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/1709.074172">
                <b>Neural Optimizer Search with Reinforcement Learning</b>               
              </a>
              <br>
              <i> <strong>Irwan Bello*</strong>, Barret Zoph*, Vijay Vasudevan, Quoc Le.</i>
              <br>
              [ICML 2017]
              <a href="https://ai.googleblog.com/2018/03/using-machine-learning-to-discover.html">
                [Google AI blogpost]                  
              </a>
              <p></p>
              <p>
                Automated discovery of optimization methods by generating update rules with an RL-trained controller.
                Discovered two new optimizers and learning rate schedules which experimentally lead to faster convergence in image classification and machine translation.
              </p>
            </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/1611.09940">
                <b>Neural Combinatorial Optimization with Reinforcement Learning</b>                 
              </a>
              <br>
              <i> <strong>Irwan Bello*</strong>, Hieu Pham*, Quoc Le, Mohammad Norouzi, Samy Bengio.</i>
              <p></p>
              <p>
                A framework to tackle combinatorial optimization problems using neural networks and reinforcement learning.
                It has since been the topic of a <a href="https://www.math.uwaterloo.ca/~bico/co759/2018/index.html">course by William J Cook</a> and been applied to
                <a href="https://arxiv.org/abs/1802.04240">Vehicle Routing</a>, 
                <a href="https://arxiv.org/abs/1804.06896">3D Bin Packing</a>, 
                <a href="https://arxiv.org/abs/1706.04972">Device Placement</a>, 
                <a href="https://arxiv.org/abs/1803.00693">E-Commerce Search Engine Ranking</a>
              </p>
            </p>
          </td>
        </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right"><font size="2">
                <a href="http://www.cs.berkeley.edu/~barron/">(website template credits)</a>
                </font>
              </p>
            </td>
          </tr>
        </table>

      </td>
    </tr>
  </table>
</body>

</html>
