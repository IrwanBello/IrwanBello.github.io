<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Irwan Bello</title>
  
  <meta name="author" content="Irwan Bello">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Irwan Bello</name>
              </p>
              <p>
                I'm a Research Scientist at <a href="https://research.google/teams/brain/"> Google Brain </a> where I work on Artificial Intelligence.
              </p>
              <p>
                My research currently focuses on making large-scale models cheaper to work with - via sparsity, adaptive computation and better distributed computing infrastructure - with applications in language and vision.
                At Google, I've also contributed to products such as <a href="https://cloud.google.com/automl">AutoML</a> and <a href="https://blog.google/products/search/introducing-mum/">MuM</a>.
                <br>
                <br>
                I advise a few select startups with their AI/ML needs and help with fashion/music projects (<i>reach out if you're interested!</i>)
                <br>
                <br>
                Before Google, I spent wonderful years at Stanford as a grad student between the stats and the CS departments, after obtaining my M.S in Applied Math at Ecole Centrale Paris.
                <br>
                <br>
                When I'm not thinking quantitavely you can probably find me doing something music related.
                <br>
                <br>
                (Last update: September 2021)
                <br>
                <br>

              </p>
              <p style="text-align:center">
                <a href="mailto:firstname.lastname@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=mY6p8gcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/IrwanBello">Twitter</a> &nbsp/&nbsp
              <a href="https://soundcloud.com/irwan-bello">Soundcloud</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr bgcolor="#ffffd0">
          <td>
            <p>
              <a href="coming soon">
              <b>The Design Of Effective Sparse Expert Models <font color="gray">[coming soon]</font></b>
              </a>
              <br> 
              <i>Barret Zoph*, William Fedus*, <strong>Irwan Bello*</strong>, Noam Shazeer.</i>
              <p></p>
              <p>
              Sparse models such as Mixture of Experts suffer from training instabilities and finetuning issues at scale.
              We design improved methods for modeling, pretraining and finetuning sparse models and successfully finetune the largest sparse encoder-decoder model ever trained.
              </p>
            </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/2109.01696">
                <b>Revisiting 3D ResNets for Video Recognition</b>
              </a>
              <br>
              <i>Xianzhi Du, Yeqing Li, Yin Cui, Rui Qian, Jing Li, <strong>Irwan Bello</strong>.</i>
              <p></p>
              <p>
              3D ResNet-RS, obtained through improved training and scaling strategies, achieves competitive performance on Kinetics and a large Web Video Text dataset.
              </p>
            </p>
          </td>
        </tr>

        <tr bgcolor="#ffffd0">
          <td>
            <p>
              <a href="https://arxiv.org/abs/2103.07579">
                <b>Revisiting ResNets: Improved Training and Scaling Strategies</b> 
              </a>
              <br>
              <i><strong>Irwan Bello</strong>, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, Barret Zoph.</i>
              <br>
              <font color="dark green">[Neurips 2021 Spotlight]</font>
              <a href="https://github.com/tensorflow/tpu/tree/master/models/official/resnet">[Github]</a>
              <a href="https://cloud.google.com/tpu/docs/tutorials/resnet-rs-2.x">[Google Cloud]</a>
              <a href="https://wandb.ai/wandb_fc/pytorch-image-models/reports/Revisiting-ResNets-Improved-Training-and-Scaling-Strategies--Vmlldzo2NDE3NTM">[Blog posts 1</a>,
              <a href="https://gdude.de/blog/2021-03-15/Revisiting-Resnets">2</a>,
              <a href="https://andlukyane.com/blog/paper-review-resnetsr">3]</a>
              <p></p>
              <p>
                This paper disentangles the impact of architectures vs training and scaling - revealing that improvements in image classification have been primarily driven by improved training and scaling.
                Identifies general scaling strategies that improve vision models across training setups and introduces SOTA competitive ResNet-RS.
                <br> The training and scaling strategies have been used in multiple recent architectures
                [<a href="https://arxiv.org/abs/2102.08602">1</a>,
                 <a href="https://arxiv.org/abs/2102.06171">2</a>,
                 <a href="https://arxiv.org/abs/2101.11605">3</a>]
                 and the work has inspired follow-up research on scaling and regularizing architectures, e.g.
                <a href="https://arxiv.org/abs/2107.00057">RetinaNet-RS</a>,
                <a href="https://arxiv.org/abs/2109.01696">3D-ResNet-RS</a>.
              </p>
            </p>
          </td>
        </tr>

        <tr bgcolor="#ffffd0">
          <td>
            <p>
              <a href="https://arxiv.org/abs/2102.08602">
                <b>LambdaNetworks: Modeling Long-Range Interactions without Attention</b>
              </a> 
              <br>
              <i><strong>Irwan Bello.</strong></i>
              <br>
              <font color="dark green">[ICLR 2021 Spotlight]</font>
              <a href="https://github.com/lucidrains/lambda-networks">
                [Github]
              </a>
              <a href="https://www.youtube.com/watch?v=3qxJ2WD8p4w">
                [Yannic Kilcher's review]
              </a>
              <a href="https://www.youtube.com/watch?v=l5ab290no8c&t=8s">
                [London ML Meetup talk]
              </a>
              <a href="https://medium.com/analytics-vidhya/lambdanetworks-modeling-long-range-interactions-without-attention-337771f42b6f">[Blog posts 1,</a>
              <a href="https://medium.com/syncedreview/iclr-2021-submission-lambda-networks-achieve-sota-accuracy-save-massive-memory-c9eba4be057d"> 2,</a>
              <a href="https://vaclavkosar.com/ml/Lamda-Networks-Transform-Self-Attention"> 3]
              </a>
              <br> <i>This paper sits at the intersection of linear attention and fully-attentional vision models (concurrent to Vision Transformers).</i>
              <p></p>
              <p>
                Introduces <i>lambda layers: a scalable alternative to self-attention</i>.
                 Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs. 
                LambdaResNets are <b>3.2 - 4.4x</b> faster than EfficientNets in supervised learning, and <b>~9x</b> than EfficientNet and ViT in large-scale semi-supervised learning.
              </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/2010.03019">              
                <b>Global Self-Attention Networks for Image Recognition</b> 
              </a> 
              <br>
              <i><strong>Irwan Bello*</strong>, Zhuoran Shen*, Raviteja Vemulapalli, Xuhui Jia, Ching-Hui Chen.</i>
              <p></p>
              <p>
                Combining linear attention and axial attention yields an attention mechanism that can efficiently attend to higher resolution images.
              </p>
            </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/1906.05909">              
                <b>Stand-alone Self-Attention in Vision Models</b> 
              </a>
              <br>
              <i> Prajit Ramachandran*, Niki Parmar*, Ashish Vaswani*, <strong>Irwan Bello</strong>, Anselm Levskaya, Jonathon Shlens.</i>
              <br>
              [NeurIPS 2019]
              <p></p>
              <p>
                Study of fully atttentional networks on image classification and object detection.
                A simple procedure of replacing all spatial convolutions with self-attention in ResNets produces a fully self-attentional model that outperforms its convolutional counterpart on image classification and object detection, while being more computationally efficient.
                These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.
              </p>
            </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/1904.09925">
                <b>Attention Augmented Convolutional Networks</b> 
              </a>
              <br>
              <i> <strong>Irwan Bello</strong>, Barret Zoph, Ashish Vaswani, Jonathon Shlens, Quoc Le.</i>
              <br>
              [ICCV 2019]
              <p></p>
              <p>
                Trained <i>the first fully attentional image classifier</i> and showed that self-attention is a competitive replacement to convolutions for image classification.
                Hybrid architectures which combine self-attention and convolution yields sizable improvements on image classification and object detection.
              </p>
            </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/1810.02019">
                <b>Seq2slate: Re-ranking and Slate Optimization with RNNs</b>                 
              </a>
              <br>
              <i> <strong>Irwan Bello</strong>, Sayali Kulkarni, Sagar Jain, Craig Boutilier, Ed Chi, Elad Eban, Xiyang Luo, Alan Mackey, Ofer Meshi.</i>
              <p></p>
              <p>
                Learning to rank with Pointer Networks outperforms pointwise, pairwise and listwise ranking baselines on academic datasets and in offline experiments.
              </p>
            </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/1808.02822">
                <b>Backprop Evolution</b>
              </a>
              <br>
              <i> Maximilian Alber*, <strong>Irwan Bello*</strong>, Barret Zoph, Pieter-Jan Kindermans, Prajit Ramachandran, Quoc Le.</i>
              <p></p>
              <p>
                Starting from random or known propagation rules, evolution searches for backpropagation variants that maximize generalization performance.
              </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/1709.074172">
                <b>Neural Optimizer Search with Reinforcement Learning</b>               
              </a>
              <br>
              <i> <strong>Irwan Bello*</strong>, Barret Zoph*, Vijay Vasudevan, Quoc Le.</i>
              <br>
              [ICML 2017]
              <a href="https://ai.googleblog.com/2018/03/using-machine-learning-to-discover.html">
                [Google AI blogpost]                  
              </a>
              <p></p>
              <p>
                Automated discovery of optimization methods by generating update rules with an RL-trained controller.
                Discovered two new optimizers and learning rate schedules which experimentally lead to faster convergence in image classification and machine translation.
              </p>
            </p>
          </td>
        </tr>

        <tr>
          <td>
            <p>
              <a href="https://arxiv.org/abs/1611.09940">
                <b>Neural Combinatorial Optimization with Reinforcement Learning</b>                 
              </a>
              <br>
              <i> <strong>Irwan Bello*</strong>, Hieu Pham*, Quoc Le, Mohammad Norouzi, Samy Bengio.</i>
              <p></p>
              <p>
                A framework to tackle combinatorial optimization problems using neural networks and reinforcement learning.
                It has since been the topic of a <a href="https://www.math.uwaterloo.ca/~bico/co759/2018/index.html">course by William J Cook</a> and been applied to
                <a href="https://arxiv.org/abs/1802.04240">Vehicle Routing</a>, 
                <a href="https://arxiv.org/abs/1804.06896">3D Bin Packing</a>, 
                <a href="https://arxiv.org/abs/1706.04972">Device Placement</a>, 
                <a href="https://arxiv.org/abs/1803.00693">E-Commerce Search Engine Ranking</a>
              </p>
            </p>
          </td>
        </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right"><font size="2">
                <a href="http://www.cs.berkeley.edu/~barron/">(website template credits)</a>
                </font>
              </p>
            </td>
          </tr>
        </table>

      </td>
    </tr>
  </table>
</body>

</html>
